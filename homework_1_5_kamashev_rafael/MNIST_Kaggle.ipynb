{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Содание модели, предсказывающей буквы латинского алфавита на основе базы EMNIST.\n",
    "\n",
    "1. Предобработка базы изображений (удаление неинформативных пикселей, уменьшение значений в пределах 0...1, нормализация с помощью StandardScaler)\n",
    "2. Поиск наиболее подходящего классификатора\n",
    "3. Подбор наиболее подходящих гиперпараметров нескольких наилучших классификаторов\n",
    "4. Получение результатов на тестовой выборке\n",
    "5. Попытка снизить размерность модели с помощью PCA.\n",
    "6. Отдельная попытка, неудачная: векторизация изображений https://colab.research.google.com/drive/13sTcHhWMHVAnNm5XqxOj9V-SITQ7ERZo?usp=sharing В итоге потом вернулся к работе над данным ноутбуком\n",
    "\n",
    "P.S. Ячеек мало, так как многие неудачные эксперименты почистил. Отчет в последней ячейке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка среды"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NR-nPqESkpfZ",
    "outputId": "c8dd8aa5-0100-4b9e-8013-f2bfcca4a9b0"
   },
   "outputs": [],
   "source": [
    "!pip install -U kaggle scikit-learn==1.0 scikit-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oujfQMEWLxL5"
   },
   "outputs": [],
   "source": [
    "!touch /content/kaggle.json\n",
    "!chmod 600 /content/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ame_LJdYQ62A"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "os.environ['KAGGLE_CONFIG_DIR'] = '/content/'\n",
    "api_token = {'username': 'rafaelkamashev', 'key': 'e6891d0b2b04d3bd70bcc194f6157081'}\n",
    "\n",
    "with open('/content/kaggle.json', 'w') as file:\n",
    "    json.dump(api_token, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jwj0YJUEScAV",
    "outputId": "c5ca9194-7d5e-4bc3-e5c0-6c3c7124abd9"
   },
   "outputs": [],
   "source": [
    "!kaggle competitions download -c jds3 --force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yMCzcfu4PTgi",
    "outputId": "2eb133ad-2c36-4b48-c304-a07de64f9021"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jO6Hz2V2JvTc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'imported'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import string\n",
    "import warnings\n",
    "\n",
    "from skimage.filters import threshold_otsu\n",
    "from skimage.morphology import closing, square\n",
    "from skimage.morphology import skeletonize\n",
    "from skimage.transform import probabilistic_hough_line\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import (\n",
    "    AdaBoostClassifier, BaggingClassifier, ExtraTreesClassifier, GradientBoostingClassifier,\n",
    "    HistGradientBoostingClassifier, RandomForestClassifier, StackingClassifier, VotingClassifier\n",
    ")\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection._search_successive_halving import HalvingGridSearchCV\n",
    "from sklearn.model_selection import cross_val_score, KFold, train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "'imported'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подготовка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-wLhfUkPMRjE",
    "outputId": "f2efc336-a941-4625-f721-e7e2c930c8c3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14800, 784)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv('jds3/emnist-letters-test-sh.csv', header=None).values\n",
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lkiUi_SIMRkv",
    "outputId": "a95fb8f0-9275-4e37-ee35-32422486e5d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.56 s, sys: 240 ms, total: 3.8 s\n",
      "Wall time: 3.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(88800, 785)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "train_df = pd.read_csv('jds3/emnist-letters-train.csv.zip', header=None)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "id": "_9Sc0elRMRm-",
    "outputId": "be380bfe-86da-40bc-9a06-be3219d719fc"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>775</th>\n",
       "      <th>776</th>\n",
       "      <th>777</th>\n",
       "      <th>778</th>\n",
       "      <th>779</th>\n",
       "      <th>780</th>\n",
       "      <th>781</th>\n",
       "      <th>782</th>\n",
       "      <th>783</th>\n",
       "      <th>784</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   0    1    2    3    4    5    6    7    8    9    ...  775  776  777  778  \\\n",
       "0   23    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "1    7    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "2   16    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "3   15    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "4   23    0    0    0    0    0    0    0    0    0  ...    0    0    0    0   \n",
       "\n",
       "   779  780  781  782  783  784  \n",
       "0    0    0    0    0    0    0  \n",
       "1    0    0    0    0    0    0  \n",
       "2    0    0    0    0    0    0  \n",
       "3    0    0    0    0    0    0  \n",
       "4    0    0    0    0    0    0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YFKkqfJrPgmc",
    "outputId": "8669659c-dd46-4af6-973c-2a8eda8a525c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((88800,), (88800, 784))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y, X = train_df[0].values, train_df.drop(0, axis=1)\n",
    "X.rename(columns={col + 1: col for col in (X.columns - 1)}, inplace=True)\n",
    "(y.shape, X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nyMuEJBiPgoi",
    "outputId": "4145d108-b75a-4c54-ddcd-79504023b133"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'abcdefghijklmnopqrstuvwxyz'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alphabet = list(string.ascii_lowercase)\n",
    "string.ascii_lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 299
    },
    "id": "yQHBaCRPPgq-",
    "outputId": "9950ce46-9105-4749-d210-efd09010f245"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y =  u\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7faa4d8f2880>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "idx = np.random.randint(0, X.shape[0])\n",
    "print('Y = ', alphabet[y[idx] - 1])\n",
    "plt.imshow(X.loc[idx].values.reshape(28, 28).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "MinHsedMPgtZ"
   },
   "outputs": [],
   "source": [
    "def remove_pixels(df):\n",
    "  \"\"\"\n",
    "  Функция убирает неинформативные пиксели, в которых не было ни одного значения во всех примерах,\n",
    "  либо такое значение было на максимуме\n",
    "  \"\"\"\n",
    "\n",
    "  changed_df = df.loc[:]\n",
    "  dropped_px = []\n",
    "\n",
    "  for col in df:\n",
    "    if changed_df[col].max() <= 0 or changed_df[col].min() >= 255:\n",
    "      changed_df.drop(columns=[col], inplace=True)\n",
    "      dropped_px.append(col)\n",
    "  \n",
    "  return changed_df, dropped_px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZEAoYDwZMRpX",
    "outputId": "49c06cbf-18a7-4674-cbdb-837dc91fa307"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2.26 s, sys: 2.18 s, total: 4.44 s\n",
      "Wall time: 4.44 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(88800, 737)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "new_train_df, dropped = remove_pixels(X)\n",
    "new_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JgmZaGQLXX1u",
    "outputId": "4b60bb96-0a4a-4cba-877a-9e1b08b4b9d5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88800, 737)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = new_train_df.values\n",
    "# следующее напрасно: потеря информации, уменьшение качества предсказания на 3-5%\n",
    "# Возможно отсекать нужно жестче, учитывая, что в MNIST не шумов\n",
    "# на мой взгляд, лучше уменьшить размерность в 255 раз и обработать через StandardScaler\n",
    "# X_train[X_train < 128] = 0\n",
    "# X_train[X_train >= 128] = 255\n",
    "\n",
    "# приводим к размерности .0 ... .1\n",
    "X_train = X_train / 255.\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_OATfeDQXvcR",
    "outputId": "e7afe2e7-392c-4090-bce4-9dcce230ed91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(88800, 737)\n",
      "CPU times: user 2.86 s, sys: 298 ms, total: 3.16 s\n",
      "Wall time: 3.15 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(88800, 737)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train_scaled = sc.fit_transform(X_train)\n",
    "print(X_train_scaled.shape)\n",
    "\n",
    "vectorizer = TfidfTransformer()\n",
    "X_train_tf_idf = vectorizer.fit_transform(X_train)\n",
    "X_train_tf_idf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14800, 737)\n"
     ]
    }
   ],
   "source": [
    "# # убираем колонки пикселей в тестовой, которые удалили из тренировочной выборки\n",
    "X_test_dropped = np.delete(test_df, dropped, 1)\n",
    "print(X_test_dropped.shape)\n",
    "\n",
    "# также аналогично с тренировочной выборкой нормализуем\n",
    "X_test_scaled = X_test_dropped / 255.\n",
    "X_test_scaled = sc.transform(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Предварительная оценка эффективности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nxxKQ_z4hRDS",
    "outputId": "e51f488c-271d-47cc-eabc-9e5d0e76c4be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 737)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = 20000\n",
    "# берем 20 тыс. из тренировочной выборки для предварительной проверки гипотез\n",
    "X_train_small, X_train_scaled_small, y_train_small = X_train[:batch], X_train_scaled[:batch], y[:batch]\n",
    "X_train_scaled_small.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r1VoEgfRhRFp",
    "outputId": "0cdab387-ed24-4bfe-c8cf-0c4f9cb8d0eb"
   },
   "outputs": [],
   "source": [
    "folds = 10\n",
    "random_state = 10\n",
    "verbose = 1  # 0\n",
    "\n",
    "# закомментрированные показали слишком низкий результат при первых попытках.\n",
    "# Одиночные модели SVC, KNeighbors тоже проверял по многу раз, пробую ансамблевые\n",
    "\n",
    "#     'DecisionTreeClassifier': DecisionTreeClassifier(max_depth=8),\n",
    "#     'LogisticRegression': LogisticRegression(multi_class='multinomial'),\n",
    "#     'BernoulliNB': BernoulliNB(),\n",
    "#     'AdaBoostDT': AdaBoostClassifier(\n",
    "#         DecisionTreeClassifier(max_depth=8, min_samples_split=4, random_state=random_state),\n",
    "#         n_estimators=50, random_state=random_state\n",
    "#     ),\n",
    "#     'BaggingDT': BaggingClassifier(\n",
    "#         DecisionTreeClassifier(max_depth=8, min_samples_split=4, random_state=random_state),\n",
    "#         n_estimators=10, random_state=random_state, verbose=verbose\n",
    "#     ),\n",
    "#     'GradientBoosting': GradientBoostingClassifier(\n",
    "#         min_samples_split=4, n_estimators=50, random_state=random_state, verbose=verbose\n",
    "#     ),\n",
    "\n",
    "models = {\n",
    "    'ExtraTrees': ExtraTreesClassifier(\n",
    "        max_depth=10, min_samples_split=5, n_estimators=100, random_state=random_state, verbose=verbose\n",
    "    ),\n",
    "    'HistGradientBoosting': HistGradientBoostingClassifier(random_state=random_state, verbose=verbose),\n",
    "    'BaggingKN': BaggingClassifier(\n",
    "        KNeighborsClassifier(n_neighbors=10),\n",
    "        n_estimators=100, random_state=random_state, verbose=verbose\n",
    "    ),\n",
    "    'BaggingSVC': BaggingClassifier(\n",
    "        SVC(C=1.2, gamma=.5, kernel='rbf', random_state=random_state, cache_size=1024),\n",
    "        n_estimators=10, random_state=random_state, verbose=verbose\n",
    "    )\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "  print(name, '... ')\n",
    "  k_fold = KFold(n_splits=folds)\n",
    "  res = cross_val_score(\n",
    "      model, X_train_scaled_small, y_train_small, cv=k_fold, scoring='accuracy', n_jobs=-1, verbose=verbose\n",
    "  )\n",
    "  print(name, res, res.mean())\n",
    "  results[name] = res.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Не дотерпел. Запускать на слабом ноутбуке 10 классификаторов SVC параллельно с беггингом было ошибкой. Скопирую лог показателей точности кросс-валидации:\n",
    "```\n",
    "GradientBoosting [0.697  0.7065 0.695  0.719  0.714  0.71   0.7005 0.704  0.7225 0.7145] 0.7083\n",
    "BaggingKN [0.7415 0.7515 0.7465 0.7555 0.7535 0.7515 0.732  0.759  0.768  0.762 ] 0.75209\n",
    "BaggingDT [0.639  0.638  0.629  0.656  0.6385 0.64   0.627  0.6235 0.645  0.654 ] 0.639\n",
    "AdaBoostDT [0.528  0.5105 0.5305 0.4985 0.547  0.5095 0.5035 0.5085 0.512  0.5165] 0.51645\n",
    "HistGradientBoosting [0.8215 0.834  0.8265 0.83   0.8245 0.829  0.821  0.826  0.841  0.8435] 0.829699\n",
    "ExtraTrees [0.7555 0.7605 0.752  0.7675 0.777  0.768  0.7555 0.748  0.77   0.7715] 0.762549\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SIFTR-heE_0X",
    "outputId": "e2280b32-4340-4890-de26-97fad4af866b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "extra_trees...\n",
      "n_iterations: 9\n",
      "n_required_iterations: 9\n",
      "n_possible_iterations: 12\n",
      "min_resources_: 30\n",
      "max_resources_: 88800\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 360\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 360 candidates, totalling 1800 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 180\n",
      "n_resources: 60\n",
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 90\n",
      "n_resources: 120\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 45\n",
      "n_resources: 240\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 23\n",
      "n_resources: 480\n",
      "Fitting 5 folds for each of 23 candidates, totalling 115 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 12\n",
      "n_resources: 960\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 6\n",
      "n_resources: 1920\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 3\n",
      "n_resources: 3840\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----------\n",
      "iter: 8\n",
      "n_candidates: 2\n",
      "n_resources: 7680\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n",
      "!!!!!! clf= extra_trees , input= scaled_sample , best_params_ {'criterion': 'entropy', 'max_depth': 15, 'max_features': 'auto', 'min_samples_split': 5, 'n_estimators': 400} , best_score_ 0.7951822916666667 , best_estimator_ ExtraTreesClassifier(criterion='entropy', max_depth=15, min_samples_split=5,\n",
      "                     n_estimators=400, n_jobs=-1, random_state=10)\n",
      "n_iterations: 9\n",
      "n_required_iterations: 9\n",
      "n_possible_iterations: 12\n",
      "min_resources_: 30\n",
      "max_resources_: 88800\n",
      "aggressive_elimination: False\n",
      "factor: 2\n",
      "----------\n",
      "iter: 0\n",
      "n_candidates: 360\n",
      "n_resources: 30\n",
      "Fitting 5 folds for each of 360 candidates, totalling 1800 fits\n",
      "----------\n",
      "iter: 1\n",
      "n_candidates: 180\n",
      "n_resources: 60\n",
      "Fitting 5 folds for each of 180 candidates, totalling 900 fits\n",
      "----------\n",
      "iter: 2\n",
      "n_candidates: 90\n",
      "n_resources: 120\n",
      "Fitting 5 folds for each of 90 candidates, totalling 450 fits\n",
      "----------\n",
      "iter: 3\n",
      "n_candidates: 45\n",
      "n_resources: 240\n",
      "Fitting 5 folds for each of 45 candidates, totalling 225 fits\n",
      "----------\n",
      "iter: 4\n",
      "n_candidates: 23\n",
      "n_resources: 480\n",
      "Fitting 5 folds for each of 23 candidates, totalling 115 fits\n",
      "----------\n",
      "iter: 5\n",
      "n_candidates: 12\n",
      "n_resources: 960\n",
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "----------\n",
      "iter: 6\n",
      "n_candidates: 6\n",
      "n_resources: 1920\n",
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "----------\n",
      "iter: 7\n",
      "n_candidates: 3\n",
      "n_resources: 3840\n",
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n",
      "----------\n",
      "iter: 8\n",
      "n_candidates: 2\n",
      "n_resources: 7680\n",
      "Fitting 5 folds for each of 2 candidates, totalling 10 fits\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_358095/636372750.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0minput_name\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0minput_\u001B[0m \u001B[0;32min\u001B[0m \u001B[0minputs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mitems\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m         search = HalvingGridSearchCV(\n\u001B[0m\u001B[1;32m     40\u001B[0m             \u001B[0mclf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mparams\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcv\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfactor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmin_resources\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m30\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mn_jobs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mverbose\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m         ).fit(input_, y)\n",
      "\u001B[0;32m~/ds/data-science/venv/lib/python3.8/site-packages/sklearn/model_selection/_search_successive_halving.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[1;32m    260\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_n_samples_orig\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_num_samples\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    261\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 262\u001B[0;31m         \u001B[0msuper\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mgroups\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mgroups\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    263\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    264\u001B[0m         \u001B[0;31m# Set best_score_: BaseSearchCV does not set it, as refit is a callable\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ds/data-science/venv/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, groups, **fit_params)\u001B[0m\n\u001B[1;32m    924\u001B[0m             \u001B[0mrefit_start_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    925\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0my\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 926\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbest_estimator_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0my\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    927\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    928\u001B[0m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbest_estimator_\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mX\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mfit_params\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ds/data-science/venv/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001B[0m in \u001B[0;36mfit\u001B[0;34m(self, X, y, sample_weight)\u001B[0m\n\u001B[1;32m    439\u001B[0m             \u001B[0;31m# parallel_backend contexts set at a higher level,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    440\u001B[0m             \u001B[0;31m# since correctness does not rely on using threads.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 441\u001B[0;31m             trees = Parallel(\n\u001B[0m\u001B[1;32m    442\u001B[0m                 \u001B[0mn_jobs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mn_jobs\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    443\u001B[0m                 \u001B[0mverbose\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mverbose\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ds/data-science/venv/lib/python3.8/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, iterable)\u001B[0m\n\u001B[1;32m   1052\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1053\u001B[0m             \u001B[0;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mretrieval_context\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1054\u001B[0;31m                 \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mretrieve\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1055\u001B[0m             \u001B[0;31m# Make sure that we get a last message telling us we are done\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1056\u001B[0m             \u001B[0melapsed_time\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mtime\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtime\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m-\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_start_time\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/ds/data-science/venv/lib/python3.8/site-packages/joblib/parallel.py\u001B[0m in \u001B[0;36mretrieve\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    931\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    932\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mgetattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'supports_timeout'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 933\u001B[0;31m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    934\u001B[0m                 \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    935\u001B[0m                     \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_output\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mextend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mjob\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.8/multiprocessing/pool.py\u001B[0m in \u001B[0;36mget\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    763\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    764\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 765\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    766\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mready\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    767\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mTimeoutError\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.8/multiprocessing/pool.py\u001B[0m in \u001B[0;36mwait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    760\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    761\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 762\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_event\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    763\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    764\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.8/threading.py\u001B[0m in \u001B[0;36mwait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    556\u001B[0m             \u001B[0msignaled\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_flag\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    557\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0msignaled\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 558\u001B[0;31m                 \u001B[0msignaled\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_cond\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    559\u001B[0m             \u001B[0;32mreturn\u001B[0m \u001B[0msignaled\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    560\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.8/threading.py\u001B[0m in \u001B[0;36mwait\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    300\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m    \u001B[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    301\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mtimeout\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 302\u001B[0;31m                 \u001B[0mwaiter\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0macquire\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    303\u001B[0m                 \u001B[0mgotit\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    304\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "# выберем лучшие гиперпараметры\n",
    "random_state = 10\n",
    "verbose = 1\n",
    "clfs = {\n",
    "    'extra_trees': {\n",
    "        'clf': ExtraTreesClassifier(random_state=random_state, verbose=0, n_jobs=-1),\n",
    "        'params': {\n",
    "            'n_estimators': [100, 200, 300, 400],\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': [5, 8, 10, 15, 25],\n",
    "            'min_samples_split': [3, 5, 10],\n",
    "            'max_features': ['auto', 'sqrt', 'log2']\n",
    "        }\n",
    "        \n",
    "    },\n",
    "# выполнялся крайне долго. Решил подобрать параметры вручную\n",
    "#     'hist_gradient_boosting': {\n",
    "#         'clf': HistGradientBoostingClassifier(random_state=random_state, verbose=0),\n",
    "#         'params': {\n",
    "#             'l2_regularization': [0., 0.5, 1., 1.5],\n",
    "#             'max_iter': [1000, 1200, 1500],\n",
    "#             'max_leaf_nodes': [15, 31, 50],\n",
    "#             'max_depth': [10, 25, 50, 75],\n",
    "#             'min_samples_leaf': [5, 15, 30]\n",
    "#         }\n",
    "#     }\n",
    "}\n",
    "\n",
    "inputs = {\n",
    "    'scaled_sample': X_train_scaled,\n",
    "    'tf_idf': X_train_tf_idf\n",
    "}\n",
    "\n",
    "grids = {}\n",
    "for classifier_name, config in clfs.items(): \n",
    "    print(f'\\n{classifier_name}...')\n",
    "    clf, params = config['clf'], config['params']\n",
    "\n",
    "    for input_name, input_ in inputs.items():\n",
    "        search = HalvingGridSearchCV(\n",
    "            clf, params, cv=5, factor=2, min_resources=30, n_jobs=-1, verbose=verbose\n",
    "        ).fit(input_, y)\n",
    "\n",
    "        grids[(classifier_name, input_name)] = search\n",
    "\n",
    "        print(\n",
    "            '!!!!!!',\n",
    "            'clf=', classifier_name,\n",
    "            ', input=', input_name,\n",
    "            ', best_params_', search.best_params_,\n",
    "            ', best_score_', search.best_score_,\n",
    "            ', best_estimator_', search.best_estimator_\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "7NTluQHtE_8Z"
   },
   "outputs": [],
   "source": [
    "def do_pca(n_components, data):\n",
    "  \"\"\"Уменьшение размерности выборки с помощью PCA\"\"\"\n",
    "  pca = PCA(n_components)\n",
    "  X_pca = pca.fit_transform(data)\n",
    "\n",
    "  return pca, X_pca\n",
    "\n",
    "\n",
    "def fit_classifier(X_train, X_test, y_train, y_val, clf=None, classifier=SVC, clf_params=None, print_output=True):\n",
    "  \"\"\"Функция трениовки модели и проверки на валидирующей выборке\"\"\"\n",
    "  if clf_params is None:\n",
    "    clf_params = {}\n",
    "\n",
    "  if clf is None:\n",
    "    clf = classifier(n_jobs=-1, **clf_params)\n",
    "  clf.fit(X_train, y_train)\n",
    "\n",
    "  y_preds = clf.predict(X_test)\n",
    "  acc = accuracy_score(y_val, y_preds)\n",
    "\n",
    "  if print_output == True:\n",
    "      mat = confusion_matrix(y_val, y_preds)\n",
    "      sns.heatmap(mat, annot=True, cmap='bwr', linewidths=.5)\n",
    "\n",
    "      print('Input Shape: {}'.format(X_train.shape))\n",
    "      print('Accuracy: {:2.2%}\\n'.format(acc))\n",
    "      print(mat)\n",
    "\n",
    "  return acc, clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZvgsToXqe3dO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71040, 737) (17760, 737) (71040,) (17760,)\n",
      "Accuracy 0.855518018018018\n"
     ]
    }
   ],
   "source": [
    "# разбираем тренировочный сет на тренировочный и валидирующий наборы\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y, test_size=.2, random_state=10)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "clf = RandomForestClassifier(\n",
    "    random_state=random_state, verbose=0, n_jobs=-1,\n",
    "    n_estimators=400, max_depth=12, min_samples_split=3, min_samples_leaf=5\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f'Accuracy', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA dimensions std sc: (88800, 334)\n"
     ]
    }
   ],
   "source": [
    "# валидационный прогон с учетом PCA\n",
    "n_components = 0.98  # 98%\n",
    "\n",
    "pca, X_pca = do_pca(n_components, X_train_scaled)\n",
    "print(f'PCA dimensions std sc: {X_pca.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71040, 334) (17760, 334) (71040,) (17760,)\n",
      "Accuracy 0.7596846846846846\n"
     ]
    }
   ],
   "source": [
    "# попробовал PCA, но результат стал хуже. Похоже, PCA лучше работает не в древовидных классификаторах.\n",
    "# Но они рассчитываются дольше, мне не хватит времени.\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_pca, y, test_size=.2, random_state=10)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "clf = RandomForestClassifier(\n",
    "    random_state=random_state, verbose=0, n_jobs=-1,\n",
    "    n_estimators=400, max_depth=12, min_samples_split=3, min_samples_leaf=5\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f'Accuracy', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "q5YUQfWCn6v3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71040, 737) (17760, 737) (71040,) (17760,)\n",
      "Binning 0.377 GB of training data: 2.358 s\n",
      "Binning 0.042 GB of validation data: 0.063 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/100] 26 trees, 806 leaves (31 on avg), max depth = 12, train loss: 2.12842, val loss: 2.20265, in 2.813s\n",
      "[2/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 1.81773, val loss: 1.92594, in 2.858s\n",
      "[3/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 1.60329, val loss: 1.73178, in 3.045s\n",
      "[4/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 1.43537, val loss: 1.57852, in 3.104s\n",
      "[5/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 1.29824, val loss: 1.45557, in 3.315s\n",
      "[6/100] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 1.18254, val loss: 1.35199, in 3.259s\n",
      "[7/100] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 1.08569, val loss: 1.26440, in 3.584s\n",
      "[8/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.99850, val loss: 1.18796, in 3.620s\n",
      "[9/100] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.92443, val loss: 1.12100, in 5.700s\n",
      "[10/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.85906, val loss: 1.06393, in 4.816s\n",
      "[11/100] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.80136, val loss: 1.01318, in 4.690s\n",
      "[12/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.74892, val loss: 0.96911, in 3.666s\n",
      "[13/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.70298, val loss: 0.92682, in 3.693s\n",
      "[14/100] 26 trees, 806 leaves (31 on avg), max depth = 21, train loss: 0.66167, val loss: 0.89079, in 3.716s\n",
      "[15/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.62251, val loss: 0.85592, in 3.652s\n",
      "[16/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.58775, val loss: 0.82559, in 3.655s\n",
      "[17/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.55561, val loss: 0.79809, in 3.741s\n",
      "[18/100] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 0.52681, val loss: 0.77379, in 3.733s\n",
      "[19/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.50031, val loss: 0.75274, in 3.800s\n",
      "[20/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.47599, val loss: 0.73224, in 3.755s\n",
      "[21/100] 26 trees, 806 leaves (31 on avg), max depth = 20, train loss: 0.45315, val loss: 0.71490, in 3.744s\n",
      "[22/100] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.43191, val loss: 0.69810, in 3.756s\n",
      "[23/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.41186, val loss: 0.68281, in 3.763s\n",
      "[24/100] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.39284, val loss: 0.66759, in 3.716s\n",
      "[25/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.37501, val loss: 0.65316, in 3.753s\n",
      "[26/100] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.35806, val loss: 0.63952, in 4.069s\n",
      "[27/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.34249, val loss: 0.62712, in 4.010s\n",
      "[28/100] 26 trees, 806 leaves (31 on avg), max depth = 20, train loss: 0.32797, val loss: 0.61632, in 4.325s\n",
      "[29/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.31464, val loss: 0.60688, in 5.176s\n",
      "[30/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.30145, val loss: 0.59673, in 5.155s\n",
      "[31/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.28972, val loss: 0.58785, in 6.429s\n",
      "[32/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.27774, val loss: 0.57927, in 3.961s\n",
      "[33/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.26694, val loss: 0.57186, in 3.732s\n",
      "[34/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.25629, val loss: 0.56405, in 3.830s\n",
      "[35/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.24609, val loss: 0.55725, in 3.822s\n",
      "[36/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.23652, val loss: 0.54985, in 3.839s\n",
      "[37/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.22725, val loss: 0.54351, in 3.869s\n",
      "[38/100] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.21866, val loss: 0.53748, in 3.818s\n",
      "[39/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.21048, val loss: 0.53155, in 3.812s\n",
      "[40/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.20274, val loss: 0.52653, in 3.879s\n",
      "[41/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.19555, val loss: 0.52152, in 3.846s\n",
      "[42/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.18844, val loss: 0.51619, in 3.890s\n",
      "[43/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.18158, val loss: 0.51101, in 3.883s\n",
      "[44/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.17517, val loss: 0.50612, in 3.861s\n",
      "[45/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.16886, val loss: 0.50101, in 4.166s\n",
      "[46/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.16279, val loss: 0.49656, in 3.916s\n",
      "[47/100] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.15720, val loss: 0.49244, in 3.854s\n",
      "[48/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.15175, val loss: 0.48861, in 3.870s\n",
      "[49/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.14668, val loss: 0.48531, in 3.820s\n",
      "[50/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.14179, val loss: 0.48213, in 3.945s\n",
      "[51/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.13701, val loss: 0.47900, in 4.048s\n",
      "[52/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.13250, val loss: 0.47573, in 3.852s\n",
      "[53/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.12810, val loss: 0.47218, in 3.839s\n",
      "[54/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.12396, val loss: 0.46937, in 3.950s\n",
      "[55/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.12008, val loss: 0.46624, in 3.931s\n",
      "[56/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.11617, val loss: 0.46325, in 4.141s\n",
      "[57/100] 26 trees, 806 leaves (31 on avg), max depth = 20, train loss: 0.11254, val loss: 0.46084, in 3.931s\n",
      "[58/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.10905, val loss: 0.45899, in 3.860s\n",
      "[59/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.10565, val loss: 0.45651, in 3.958s\n",
      "[60/100] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.10240, val loss: 0.45397, in 4.063s\n",
      "[61/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.09928, val loss: 0.45161, in 3.919s\n",
      "[62/100] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.09629, val loss: 0.44922, in 3.894s\n",
      "[63/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.09344, val loss: 0.44741, in 3.838s\n",
      "[64/100] 26 trees, 806 leaves (31 on avg), max depth = 21, train loss: 0.09068, val loss: 0.44557, in 3.876s\n",
      "[65/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.08797, val loss: 0.44349, in 3.895s\n",
      "[66/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.08536, val loss: 0.44131, in 3.946s\n",
      "[67/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.08276, val loss: 0.43902, in 3.969s\n",
      "[68/100] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.08029, val loss: 0.43675, in 3.923s\n",
      "[69/100] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.07806, val loss: 0.43545, in 3.905s\n",
      "[70/100] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.07581, val loss: 0.43378, in 3.808s\n",
      "[71/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.07358, val loss: 0.43209, in 3.886s\n",
      "[72/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.07150, val loss: 0.43098, in 3.909s\n",
      "[73/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.06952, val loss: 0.42937, in 4.150s\n",
      "[74/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.06752, val loss: 0.42774, in 3.858s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.06569, val loss: 0.42641, in 3.890s\n",
      "[76/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.06391, val loss: 0.42445, in 3.850s\n",
      "[77/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.06203, val loss: 0.42346, in 3.921s\n",
      "[78/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.06030, val loss: 0.42234, in 3.873s\n",
      "[79/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.05863, val loss: 0.42095, in 3.904s\n",
      "[80/100] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.05705, val loss: 0.41964, in 3.943s\n",
      "[81/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.05547, val loss: 0.41866, in 3.896s\n",
      "[82/100] 26 trees, 806 leaves (31 on avg), max depth = 20, train loss: 0.05397, val loss: 0.41731, in 3.805s\n",
      "[83/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.05251, val loss: 0.41585, in 3.909s\n",
      "[84/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.05114, val loss: 0.41474, in 3.883s\n",
      "[85/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.04977, val loss: 0.41397, in 3.798s\n",
      "[86/100] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.04850, val loss: 0.41274, in 5.347s\n",
      "[87/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.04721, val loss: 0.41163, in 4.014s\n",
      "[88/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.04599, val loss: 0.41062, in 3.845s\n",
      "[89/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.04478, val loss: 0.40960, in 3.879s\n",
      "[90/100] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.04354, val loss: 0.40853, in 3.890s\n",
      "[91/100] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.04245, val loss: 0.40738, in 3.853s\n",
      "[92/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.04140, val loss: 0.40665, in 4.718s\n",
      "[93/100] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.04031, val loss: 0.40580, in 4.431s\n",
      "[94/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.03930, val loss: 0.40496, in 4.093s\n",
      "[95/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.03838, val loss: 0.40421, in 3.855s\n",
      "[96/100] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.03734, val loss: 0.40336, in 3.868s\n",
      "[97/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.03641, val loss: 0.40292, in 3.801s\n",
      "[98/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.03555, val loss: 0.40227, in 4.018s\n",
      "[99/100] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.03467, val loss: 0.40133, in 4.279s\n",
      "[100/100] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.03379, val loss: 0.40088, in 4.140s\n",
      "Fit 2600 trees in 397.334 s, (80600 total leaves)\n",
      "Time spent computing histograms: 264.779s\n",
      "Time spent finding best splits:  45.781s\n",
      "Time spent applying splits:      5.374s\n",
      "Time spent predicting:           0.490s\n",
      "Accuracy 0.8705518018018018\n"
     ]
    }
   ],
   "source": [
    "# пробую стандартные настройки для градиентного бустинга, основанного на гистограммах\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y, test_size=.2, random_state=10)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "clf = HistGradientBoostingClassifier(\n",
    "    random_state=10, verbose=1,\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f'Accuracy', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Красота! Логи очень информативные, тренируется быстро, и начальные настройки довольно оптимальны! Попробую увеличить количество итераций, и потом потренировать через kfold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71040, 737) (17760, 737) (71040,) (17760,)\n",
      "Binning 0.377 GB of training data: 2.340 s\n",
      "Binning 0.042 GB of validation data: 0.063 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/300] 26 trees, 806 leaves (31 on avg), max depth = 12, train loss: 2.12842, val loss: 2.20265, in 2.833s\n",
      "[2/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 1.81773, val loss: 1.92594, in 2.872s\n",
      "[3/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 1.60329, val loss: 1.73178, in 3.053s\n",
      "[4/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 1.43537, val loss: 1.57852, in 3.117s\n",
      "[5/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 1.29824, val loss: 1.45557, in 4.295s\n",
      "[6/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 1.18254, val loss: 1.35199, in 4.151s\n",
      "[7/300] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 1.08569, val loss: 1.26440, in 3.990s\n",
      "[8/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.99850, val loss: 1.18796, in 4.162s\n",
      "[9/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.92443, val loss: 1.12100, in 3.892s\n",
      "[10/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.85906, val loss: 1.06393, in 3.889s\n",
      "[11/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.80136, val loss: 1.01318, in 3.978s\n",
      "[12/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.74892, val loss: 0.96911, in 4.668s\n",
      "[13/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.70298, val loss: 0.92682, in 4.200s\n",
      "[14/300] 26 trees, 806 leaves (31 on avg), max depth = 21, train loss: 0.66167, val loss: 0.89079, in 3.882s\n",
      "[15/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.62251, val loss: 0.85592, in 4.169s\n",
      "[16/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.58775, val loss: 0.82559, in 4.361s\n",
      "[17/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.55561, val loss: 0.79809, in 4.936s\n",
      "[18/300] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 0.52681, val loss: 0.77379, in 6.198s\n",
      "[19/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.50031, val loss: 0.75274, in 4.603s\n",
      "[20/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.47599, val loss: 0.73224, in 5.516s\n",
      "[21/300] 26 trees, 806 leaves (31 on avg), max depth = 20, train loss: 0.45315, val loss: 0.71490, in 5.979s\n",
      "[22/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.43191, val loss: 0.69810, in 5.454s\n",
      "[23/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.41186, val loss: 0.68281, in 4.086s\n",
      "[24/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.39284, val loss: 0.66759, in 5.376s\n",
      "[25/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.37501, val loss: 0.65316, in 5.943s\n",
      "[26/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.35806, val loss: 0.63952, in 4.630s\n",
      "[27/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.34249, val loss: 0.62712, in 4.146s\n",
      "[28/300] 26 trees, 806 leaves (31 on avg), max depth = 20, train loss: 0.32797, val loss: 0.61632, in 3.954s\n",
      "[29/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.31464, val loss: 0.60688, in 4.106s\n",
      "[30/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.30145, val loss: 0.59673, in 4.355s\n",
      "[31/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.28972, val loss: 0.58785, in 4.141s\n",
      "[32/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.27774, val loss: 0.57927, in 4.055s\n",
      "[33/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.26694, val loss: 0.57186, in 3.868s\n",
      "[34/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.25629, val loss: 0.56405, in 3.897s\n",
      "[35/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.24609, val loss: 0.55725, in 4.121s\n",
      "[36/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.23652, val loss: 0.54985, in 4.864s\n",
      "[37/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.22725, val loss: 0.54351, in 4.485s\n",
      "[38/300] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.21866, val loss: 0.53748, in 4.039s\n",
      "[39/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.21048, val loss: 0.53155, in 4.012s\n",
      "[40/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.20274, val loss: 0.52653, in 4.074s\n",
      "[41/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.19555, val loss: 0.52152, in 4.338s\n",
      "[42/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.18844, val loss: 0.51619, in 5.377s\n",
      "[43/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.18158, val loss: 0.51101, in 3.953s\n",
      "[44/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.17517, val loss: 0.50612, in 3.924s\n",
      "[45/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.16886, val loss: 0.50101, in 3.966s\n",
      "[46/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.16279, val loss: 0.49656, in 3.881s\n",
      "[47/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.15720, val loss: 0.49244, in 3.909s\n",
      "[48/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.15175, val loss: 0.48861, in 4.133s\n",
      "[49/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.14668, val loss: 0.48531, in 4.365s\n",
      "[50/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.14179, val loss: 0.48213, in 4.265s\n",
      "[51/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.13701, val loss: 0.47900, in 4.872s\n",
      "[52/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.13250, val loss: 0.47573, in 4.588s\n",
      "[53/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.12810, val loss: 0.47218, in 4.150s\n",
      "[54/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.12396, val loss: 0.46937, in 4.324s\n",
      "[55/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.12008, val loss: 0.46624, in 4.099s\n",
      "[56/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.11617, val loss: 0.46325, in 4.165s\n",
      "[57/300] 26 trees, 806 leaves (31 on avg), max depth = 20, train loss: 0.11254, val loss: 0.46084, in 4.137s\n",
      "[58/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.10905, val loss: 0.45899, in 4.184s\n",
      "[59/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.10565, val loss: 0.45651, in 5.392s\n",
      "[60/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.10240, val loss: 0.45397, in 4.091s\n",
      "[61/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.09928, val loss: 0.45161, in 3.948s\n",
      "[62/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.09629, val loss: 0.44922, in 3.892s\n",
      "[63/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.09344, val loss: 0.44741, in 3.876s\n",
      "[64/300] 26 trees, 806 leaves (31 on avg), max depth = 21, train loss: 0.09068, val loss: 0.44557, in 3.866s\n",
      "[65/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.08797, val loss: 0.44349, in 3.894s\n",
      "[66/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.08536, val loss: 0.44131, in 3.914s\n",
      "[67/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.08276, val loss: 0.43902, in 3.966s\n",
      "[68/300] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.08029, val loss: 0.43675, in 3.936s\n",
      "[69/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.07806, val loss: 0.43545, in 3.840s\n",
      "[70/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.07581, val loss: 0.43378, in 3.849s\n",
      "[71/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.07358, val loss: 0.43209, in 3.938s\n",
      "[72/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.07150, val loss: 0.43098, in 3.930s\n",
      "[73/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.06952, val loss: 0.42937, in 3.920s\n",
      "[74/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.06752, val loss: 0.42774, in 3.893s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.06569, val loss: 0.42641, in 3.894s\n",
      "[76/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.06391, val loss: 0.42445, in 3.860s\n",
      "[77/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.06203, val loss: 0.42346, in 3.952s\n",
      "[78/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.06030, val loss: 0.42234, in 4.142s\n",
      "[79/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.05863, val loss: 0.42095, in 3.978s\n",
      "[80/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.05705, val loss: 0.41964, in 3.938s\n",
      "[81/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.05547, val loss: 0.41866, in 3.904s\n",
      "[82/300] 26 trees, 806 leaves (31 on avg), max depth = 20, train loss: 0.05397, val loss: 0.41731, in 3.823s\n",
      "[83/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.05251, val loss: 0.41585, in 3.938s\n",
      "[84/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.05114, val loss: 0.41474, in 3.911s\n",
      "[85/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.04977, val loss: 0.41397, in 3.842s\n",
      "[86/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.04850, val loss: 0.41274, in 3.852s\n",
      "[87/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.04721, val loss: 0.41163, in 3.891s\n",
      "[88/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.04599, val loss: 0.41062, in 3.809s\n",
      "[89/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.04478, val loss: 0.40960, in 3.877s\n",
      "[90/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.04354, val loss: 0.40853, in 3.900s\n",
      "[91/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.04245, val loss: 0.40738, in 3.883s\n",
      "[92/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.04140, val loss: 0.40665, in 3.872s\n",
      "[93/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.04031, val loss: 0.40580, in 3.866s\n",
      "[94/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.03930, val loss: 0.40496, in 3.859s\n",
      "[95/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.03838, val loss: 0.40421, in 3.807s\n",
      "[96/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.03734, val loss: 0.40336, in 3.850s\n",
      "[97/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.03641, val loss: 0.40292, in 3.793s\n",
      "[98/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.03555, val loss: 0.40227, in 3.822s\n",
      "[99/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.03467, val loss: 0.40133, in 3.814s\n",
      "[100/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.03379, val loss: 0.40088, in 3.830s\n",
      "[101/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.03293, val loss: 0.40031, in 3.957s\n",
      "[102/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.03209, val loss: 0.39951, in 3.918s\n",
      "[103/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.03126, val loss: 0.39865, in 3.920s\n",
      "[104/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.03047, val loss: 0.39792, in 3.891s\n",
      "[105/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.02978, val loss: 0.39731, in 3.900s\n",
      "[106/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.02909, val loss: 0.39680, in 3.852s\n",
      "[107/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.02841, val loss: 0.39646, in 4.309s\n",
      "[108/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.02772, val loss: 0.39605, in 4.451s\n",
      "[109/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.02704, val loss: 0.39527, in 4.562s\n",
      "[110/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.02641, val loss: 0.39496, in 8.132s\n",
      "[111/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.02576, val loss: 0.39439, in 4.009s\n",
      "[112/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.02518, val loss: 0.39410, in 3.968s\n",
      "[113/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.02454, val loss: 0.39363, in 4.049s\n",
      "[114/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.02397, val loss: 0.39318, in 4.155s\n",
      "[115/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.02343, val loss: 0.39310, in 4.006s\n",
      "[116/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.02288, val loss: 0.39252, in 4.024s\n",
      "[117/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.02234, val loss: 0.39228, in 4.145s\n",
      "[118/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.02181, val loss: 0.39196, in 4.211s\n",
      "[119/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.02129, val loss: 0.39158, in 3.959s\n",
      "[120/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.02079, val loss: 0.39132, in 3.936s\n",
      "[121/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.02034, val loss: 0.39072, in 4.090s\n",
      "[122/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01988, val loss: 0.39040, in 4.025s\n",
      "[123/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01943, val loss: 0.38999, in 4.019s\n",
      "[124/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01902, val loss: 0.38981, in 4.001s\n",
      "[125/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01859, val loss: 0.38945, in 3.930s\n",
      "[126/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01818, val loss: 0.38904, in 3.886s\n",
      "[127/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01775, val loss: 0.38879, in 3.920s\n",
      "[128/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.01735, val loss: 0.38851, in 3.867s\n",
      "[129/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01696, val loss: 0.38802, in 3.837s\n",
      "[130/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.01658, val loss: 0.38766, in 3.874s\n",
      "[131/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01624, val loss: 0.38745, in 3.867s\n",
      "[132/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.01585, val loss: 0.38723, in 5.000s\n",
      "[133/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01550, val loss: 0.38712, in 4.369s\n",
      "[134/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01515, val loss: 0.38701, in 4.028s\n",
      "[135/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.01477, val loss: 0.38678, in 4.003s\n",
      "[136/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01443, val loss: 0.38633, in 4.110s\n",
      "[137/300] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 0.01412, val loss: 0.38633, in 4.427s\n",
      "[138/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01382, val loss: 0.38640, in 4.296s\n",
      "[139/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.01353, val loss: 0.38651, in 4.018s\n",
      "[140/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01324, val loss: 0.38623, in 3.850s\n",
      "[141/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.01296, val loss: 0.38624, in 4.050s\n",
      "[142/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01269, val loss: 0.38639, in 3.896s\n",
      "[143/300] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.01239, val loss: 0.38607, in 4.245s\n",
      "[144/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01213, val loss: 0.38630, in 4.201s\n",
      "[145/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01186, val loss: 0.38602, in 3.945s\n",
      "[146/300] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 0.01162, val loss: 0.38570, in 4.023s\n",
      "[147/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01137, val loss: 0.38571, in 4.029s\n",
      "[148/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01112, val loss: 0.38605, in 4.050s\n",
      "[149/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01090, val loss: 0.38598, in 3.892s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01068, val loss: 0.38593, in 4.349s\n",
      "[151/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.01047, val loss: 0.38571, in 4.019s\n",
      "[152/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.01022, val loss: 0.38557, in 4.032s\n",
      "[153/300] 26 trees, 806 leaves (31 on avg), max depth = 12, train loss: 0.01003, val loss: 0.38547, in 4.155s\n",
      "[154/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.00983, val loss: 0.38534, in 4.449s\n",
      "[155/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.00964, val loss: 0.38543, in 3.931s\n",
      "[156/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.00944, val loss: 0.38538, in 4.152s\n",
      "[157/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.00924, val loss: 0.38522, in 4.095s\n",
      "[158/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.00905, val loss: 0.38509, in 4.770s\n",
      "[159/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.00886, val loss: 0.38507, in 5.147s\n",
      "[160/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.00868, val loss: 0.38522, in 4.248s\n",
      "[161/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.00850, val loss: 0.38536, in 3.803s\n",
      "[162/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.00833, val loss: 0.38544, in 3.953s\n",
      "[163/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.00815, val loss: 0.38566, in 3.810s\n",
      "[164/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.00798, val loss: 0.38532, in 4.254s\n",
      "[165/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.00780, val loss: 0.38574, in 3.988s\n",
      "[166/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.00763, val loss: 0.38574, in 4.207s\n",
      "[167/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.00747, val loss: 0.38547, in 4.344s\n",
      "[168/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.00732, val loss: 0.38561, in 4.059s\n",
      "[169/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.00718, val loss: 0.38581, in 4.310s\n",
      "Fit 4394 trees in 703.750 s, (136214 total leaves)\n",
      "Time spent computing histograms: 471.025s\n",
      "Time spent finding best splits:  83.024s\n",
      "Time spent applying splits:      11.255s\n",
      "Time spent predicting:           0.897s\n",
      "Accuracy 0.8835585585585586\n"
     ]
    }
   ],
   "source": [
    "# лог очень информативен, вижу, что стандартных 100 итераций не хватает для максимизации точности\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y, test_size=.2, random_state=10)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "clf = HistGradientBoostingClassifier(\n",
    "    random_state=10, verbose=1, max_iter=300\n",
    ")\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f'Accuracy', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "xCLznqC-Kh3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14800,) [12 14 10 15 12  1  1 18  3 17]\n"
     ]
    }
   ],
   "source": [
    "y_test_pred = clf.predict(X_test_scaled)\n",
    "print(y_test_pred.shape, y_test_pred[:10])\n",
    "\n",
    "submission = pd.DataFrame({'Id': np.arange(1, X_test_scaled.shape[0] + 1), 'Category': y_test_pred})\n",
    "submission.to_csv(f'jds3/kaggle_mnist_no_pca_hist_grad_b_std_300iter.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71040, 737) (17760, 737) (71040,) (17760,)\n",
      "Binning 0.377 GB of training data: 3.361 s\n",
      "Binning 0.042 GB of validation data: 0.067 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/300] 26 trees, 806 leaves (31 on avg), max depth = 12, train loss: 2.15195, val loss: 2.21535, in 2.864s\n",
      "[2/300] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 1.82972, val loss: 1.93546, in 2.900s\n",
      "[3/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 1.60742, val loss: 1.73763, in 3.230s\n",
      "[4/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 1.43606, val loss: 1.58732, in 3.078s\n",
      "[5/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 1.29518, val loss: 1.46210, in 3.525s\n",
      "[6/300] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 1.17999, val loss: 1.35864, in 3.733s\n",
      "[7/300] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 1.08045, val loss: 1.26763, in 3.908s\n",
      "[8/300] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 0.99600, val loss: 1.19294, in 8.188s\n",
      "[9/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.92082, val loss: 1.12489, in 4.522s\n",
      "[10/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.85498, val loss: 1.06685, in 4.269s\n",
      "[11/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.79560, val loss: 1.01384, in 3.905s\n",
      "[12/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.74349, val loss: 0.96785, in 4.150s\n",
      "[13/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.69537, val loss: 0.92657, in 4.815s\n",
      "[14/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.65301, val loss: 0.88997, in 4.278s\n",
      "[15/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.61459, val loss: 0.85714, in 4.062s\n",
      "[16/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.57871, val loss: 0.82786, in 4.009s\n",
      "[17/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.54659, val loss: 0.80005, in 4.606s\n",
      "[18/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.51726, val loss: 0.77620, in 3.912s\n",
      "[19/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.49033, val loss: 0.75286, in 4.289s\n",
      "[20/300] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 0.46543, val loss: 0.73420, in 4.018s\n",
      "[21/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.44224, val loss: 0.71504, in 3.896s\n",
      "[22/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.42098, val loss: 0.69794, in 4.690s\n",
      "[23/300] 26 trees, 806 leaves (31 on avg), max depth = 12, train loss: 0.40048, val loss: 0.68182, in 5.040s\n",
      "[24/300] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 0.38214, val loss: 0.66865, in 4.660s\n",
      "[25/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.36423, val loss: 0.65570, in 4.061s\n",
      "[26/300] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 0.34815, val loss: 0.64444, in 3.838s\n",
      "[27/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.33269, val loss: 0.63266, in 5.393s\n",
      "[28/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.31843, val loss: 0.62224, in 4.411s\n",
      "[29/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.30498, val loss: 0.61187, in 4.364s\n",
      "[30/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.29215, val loss: 0.60308, in 4.250s\n",
      "[31/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.27998, val loss: 0.59450, in 3.988s\n",
      "[32/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.26869, val loss: 0.58676, in 3.895s\n",
      "[33/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.25779, val loss: 0.57877, in 4.664s\n",
      "[34/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.24765, val loss: 0.57075, in 4.548s\n",
      "[35/300] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.23776, val loss: 0.56357, in 4.184s\n",
      "[36/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.22872, val loss: 0.55727, in 4.607s\n",
      "[37/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.21980, val loss: 0.55117, in 4.064s\n",
      "[38/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.21116, val loss: 0.54563, in 5.705s\n",
      "[39/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.20320, val loss: 0.54023, in 6.425s\n",
      "[40/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.19548, val loss: 0.53440, in 6.984s\n",
      "[41/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.18813, val loss: 0.52901, in 4.070s\n",
      "[42/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.18112, val loss: 0.52414, in 4.228s\n",
      "[43/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.17446, val loss: 0.51962, in 4.903s\n",
      "[44/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.16828, val loss: 0.51516, in 4.669s\n",
      "[45/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.16220, val loss: 0.51082, in 3.924s\n",
      "[46/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.15624, val loss: 0.50596, in 3.967s\n",
      "[47/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.15077, val loss: 0.50229, in 3.915s\n",
      "[48/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.14563, val loss: 0.49876, in 4.685s\n",
      "[49/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.14054, val loss: 0.49488, in 5.891s\n",
      "[50/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.13575, val loss: 0.49155, in 4.956s\n",
      "[51/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.13114, val loss: 0.48783, in 4.210s\n",
      "[52/300] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.12684, val loss: 0.48481, in 5.314s\n",
      "[53/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.12263, val loss: 0.48213, in 4.958s\n",
      "[54/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.11855, val loss: 0.47901, in 4.125s\n",
      "[55/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.11472, val loss: 0.47575, in 3.845s\n",
      "[56/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.11097, val loss: 0.47288, in 3.874s\n",
      "[57/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.10741, val loss: 0.47055, in 3.857s\n",
      "[58/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.10403, val loss: 0.46772, in 3.899s\n",
      "[59/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.10075, val loss: 0.46550, in 3.943s\n",
      "[60/300] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.09755, val loss: 0.46258, in 3.942s\n",
      "[61/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.09455, val loss: 0.46003, in 3.931s\n",
      "[62/300] 26 trees, 806 leaves (31 on avg), max depth = 20, train loss: 0.09176, val loss: 0.45765, in 3.891s\n",
      "[63/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.08892, val loss: 0.45544, in 3.900s\n",
      "[64/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.08634, val loss: 0.45334, in 3.938s\n",
      "[65/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.08366, val loss: 0.45115, in 3.925s\n",
      "[66/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.08122, val loss: 0.44971, in 3.986s\n",
      "[67/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.07881, val loss: 0.44838, in 4.324s\n",
      "[68/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.07644, val loss: 0.44662, in 3.922s\n",
      "[69/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.07427, val loss: 0.44491, in 4.869s\n",
      "[70/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.07208, val loss: 0.44367, in 4.728s\n",
      "[71/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.06997, val loss: 0.44235, in 4.109s\n",
      "[72/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.06790, val loss: 0.44065, in 3.925s\n",
      "[73/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.06603, val loss: 0.43934, in 3.963s\n",
      "[74/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.06412, val loss: 0.43806, in 4.653s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.06241, val loss: 0.43666, in 4.930s\n",
      "[76/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.06069, val loss: 0.43536, in 5.046s\n",
      "[77/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.05899, val loss: 0.43434, in 4.673s\n",
      "[78/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.05736, val loss: 0.43314, in 3.909s\n",
      "[79/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.05579, val loss: 0.43193, in 3.864s\n",
      "[80/300] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.05421, val loss: 0.43092, in 6.216s\n",
      "[81/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.05275, val loss: 0.42992, in 4.497s\n",
      "[82/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.05131, val loss: 0.42891, in 4.115s\n",
      "[83/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.04989, val loss: 0.42739, in 4.176s\n",
      "[84/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.04852, val loss: 0.42637, in 4.692s\n",
      "[85/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.04711, val loss: 0.42552, in 3.980s\n",
      "[86/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.04593, val loss: 0.42458, in 4.117s\n",
      "[87/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.04482, val loss: 0.42380, in 4.257s\n",
      "[88/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.04359, val loss: 0.42263, in 4.682s\n",
      "[89/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.04245, val loss: 0.42163, in 4.265s\n",
      "[90/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.04136, val loss: 0.42026, in 4.129s\n",
      "[91/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.04035, val loss: 0.41948, in 4.010s\n",
      "[92/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.03934, val loss: 0.41878, in 3.843s\n",
      "[93/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.03837, val loss: 0.41765, in 3.898s\n",
      "[94/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.03736, val loss: 0.41676, in 3.966s\n",
      "[95/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.03641, val loss: 0.41621, in 4.293s\n",
      "[96/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.03551, val loss: 0.41537, in 4.039s\n",
      "[97/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.03456, val loss: 0.41504, in 4.885s\n",
      "[98/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.03368, val loss: 0.41441, in 4.093s\n",
      "[99/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.03282, val loss: 0.41389, in 4.092s\n",
      "[100/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.03200, val loss: 0.41368, in 4.350s\n",
      "[101/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.03123, val loss: 0.41293, in 3.886s\n",
      "[102/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.03042, val loss: 0.41217, in 3.844s\n",
      "[103/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.02969, val loss: 0.41165, in 3.916s\n",
      "[104/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.02897, val loss: 0.41084, in 3.959s\n",
      "[105/300] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.02827, val loss: 0.41067, in 3.904s\n",
      "[106/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.02759, val loss: 0.41013, in 3.867s\n",
      "[107/300] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.02689, val loss: 0.40998, in 3.846s\n",
      "[108/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.02624, val loss: 0.40902, in 3.904s\n",
      "[109/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.02562, val loss: 0.40871, in 3.818s\n",
      "[110/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.02503, val loss: 0.40806, in 3.914s\n",
      "[111/300] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.02434, val loss: 0.40752, in 3.957s\n",
      "[112/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.02377, val loss: 0.40706, in 3.972s\n",
      "[113/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.02322, val loss: 0.40671, in 4.043s\n",
      "[114/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.02260, val loss: 0.40639, in 3.907s\n",
      "[115/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.02203, val loss: 0.40586, in 3.952s\n",
      "[116/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.02157, val loss: 0.40562, in 3.950s\n",
      "[117/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.02110, val loss: 0.40511, in 3.804s\n",
      "[118/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.02060, val loss: 0.40455, in 3.962s\n",
      "[119/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.02016, val loss: 0.40417, in 3.929s\n",
      "[120/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01970, val loss: 0.40411, in 3.849s\n",
      "[121/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.01925, val loss: 0.40310, in 3.964s\n",
      "[122/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01880, val loss: 0.40253, in 3.908s\n",
      "[123/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.01831, val loss: 0.40217, in 4.067s\n",
      "[124/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.01792, val loss: 0.40171, in 3.738s\n",
      "[125/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01753, val loss: 0.40136, in 3.840s\n",
      "[126/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01711, val loss: 0.40115, in 3.821s\n",
      "[127/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01673, val loss: 0.40084, in 3.943s\n",
      "[128/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01637, val loss: 0.40075, in 3.848s\n",
      "[129/300] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.01600, val loss: 0.40054, in 3.851s\n",
      "[130/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01562, val loss: 0.40037, in 4.166s\n",
      "[131/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01524, val loss: 0.40037, in 3.860s\n",
      "[132/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.01491, val loss: 0.39998, in 3.860s\n",
      "[133/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01455, val loss: 0.39962, in 3.882s\n",
      "[134/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.01425, val loss: 0.39957, in 3.994s\n",
      "[135/300] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.01392, val loss: 0.39938, in 3.841s\n",
      "[136/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.01360, val loss: 0.39943, in 3.831s\n",
      "[137/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01330, val loss: 0.39934, in 3.946s\n",
      "[138/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01300, val loss: 0.39911, in 3.971s\n",
      "[139/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01270, val loss: 0.39920, in 4.021s\n",
      "[140/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01242, val loss: 0.39910, in 3.896s\n",
      "[141/300] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 0.01216, val loss: 0.39907, in 3.842s\n",
      "[142/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01190, val loss: 0.39862, in 3.912s\n",
      "[143/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.01163, val loss: 0.39888, in 3.861s\n",
      "[144/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01137, val loss: 0.39888, in 3.962s\n",
      "[145/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01113, val loss: 0.39891, in 3.817s\n",
      "[146/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01090, val loss: 0.39904, in 4.356s\n",
      "[147/300] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01066, val loss: 0.39872, in 3.844s\n",
      "[148/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.01044, val loss: 0.39882, in 3.968s\n",
      "[149/300] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01021, val loss: 0.39907, in 3.920s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[150/300] 26 trees, 806 leaves (31 on avg), max depth = 12, train loss: 0.00997, val loss: 0.39893, in 3.824s\n",
      "[151/300] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.00977, val loss: 0.39897, in 3.857s\n",
      "[152/300] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.00955, val loss: 0.39892, in 3.906s\n",
      "Fit 3952 trees in 641.254 s, (122512 total leaves)\n",
      "Time spent computing histograms: 423.012s\n",
      "Time spent finding best splits:  81.205s\n",
      "Time spent applying splits:      10.416s\n",
      "Time spent predicting:           0.824s\n",
      "Accuracy 0.8804054054054054\n"
     ]
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_tf_idf.toarray(), y, test_size=.2, random_state=10)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "clf2 = HistGradientBoostingClassifier(\n",
    "    random_state=10, verbose=1, max_iter=300, validation_fraction=None\n",
    ")\n",
    "clf2.fit(X_train, y_train)\n",
    "y_pred = clf2.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f'Accuracy', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14800,) [12  9 10 15 12  1  1 18  3  7]\n"
     ]
    }
   ],
   "source": [
    "# с tf_idf стало намного хуже.\n",
    "# Похоже, такая трансформация должна лучше работать с SVM, регрессиями, но не деревьями.\n",
    "y_test_pred = clf2.predict(X_test_scaled)\n",
    "print(y_test_pred.shape, y_test_pred[:10])\n",
    "\n",
    "submission = pd.DataFrame({'Id': np.arange(1, X_test_scaled.shape[0] + 1), 'Category': y_test_pred})\n",
    "submission.to_csv(f'jds3/kaggle_mnist_no_pca_hist_grad_b_std_idf_tf_300iter.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(75480, 737) (13320, 737) (75480,) (13320,)\n",
      "Binning 0.445 GB of training data: 2.768 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/200] 26 trees, 806 leaves (31 on avg), max depth = 13, in 3.048s\n",
      "[2/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 3.135s\n",
      "[3/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 3.306s\n",
      "[4/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 3.412s\n",
      "[5/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 3.445s\n",
      "[6/200] 26 trees, 806 leaves (31 on avg), max depth = 19, in 3.776s\n",
      "[7/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 3.884s\n",
      "[8/200] 26 trees, 806 leaves (31 on avg), max depth = 13, in 3.914s\n",
      "[9/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 3.891s\n",
      "[10/200] 26 trees, 806 leaves (31 on avg), max depth = 19, in 3.982s\n",
      "[11/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.151s\n",
      "[12/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 3.980s\n",
      "[13/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.007s\n",
      "[14/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.162s\n",
      "[15/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.020s\n",
      "[16/200] 26 trees, 806 leaves (31 on avg), max depth = 19, in 4.028s\n",
      "[17/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.045s\n",
      "[18/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.344s\n",
      "[19/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.123s\n",
      "[20/200] 26 trees, 806 leaves (31 on avg), max depth = 18, in 4.095s\n",
      "[21/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.124s\n",
      "[22/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.048s\n",
      "[23/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.076s\n",
      "[24/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.078s\n",
      "[25/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.093s\n",
      "[26/200] 26 trees, 806 leaves (31 on avg), max depth = 20, in 4.099s\n",
      "[27/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.165s\n",
      "[28/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.138s\n",
      "[29/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.121s\n",
      "[30/200] 26 trees, 806 leaves (31 on avg), max depth = 18, in 4.128s\n",
      "[31/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.113s\n",
      "[32/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.093s\n",
      "[33/200] 26 trees, 806 leaves (31 on avg), max depth = 21, in 4.126s\n",
      "[34/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.212s\n",
      "[35/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.222s\n",
      "[36/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.148s\n",
      "[37/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.189s\n",
      "[38/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.173s\n",
      "[39/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.217s\n",
      "[40/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.222s\n",
      "[41/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.213s\n",
      "[42/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.272s\n",
      "[43/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.237s\n",
      "[44/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.218s\n",
      "[45/200] 26 trees, 806 leaves (31 on avg), max depth = 18, in 4.246s\n",
      "[46/200] 26 trees, 806 leaves (31 on avg), max depth = 18, in 4.473s\n",
      "[47/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.698s\n",
      "[48/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.148s\n",
      "[49/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.191s\n",
      "[50/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.273s\n",
      "[51/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.277s\n",
      "[52/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.173s\n",
      "[53/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.199s\n",
      "[54/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.202s\n",
      "[55/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.228s\n",
      "[56/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.209s\n",
      "[57/200] 26 trees, 806 leaves (31 on avg), max depth = 18, in 4.174s\n",
      "[58/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.221s\n",
      "[59/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.250s\n",
      "[60/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.265s\n",
      "[61/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.261s\n",
      "[62/200] 26 trees, 806 leaves (31 on avg), max depth = 19, in 4.206s\n",
      "[63/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.156s\n",
      "[64/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.133s\n",
      "[65/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.298s\n",
      "[66/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.188s\n",
      "[67/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.217s\n",
      "[68/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.171s\n",
      "[69/200] 26 trees, 806 leaves (31 on avg), max depth = 18, in 5.243s\n",
      "[70/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.738s\n",
      "[71/200] 26 trees, 806 leaves (31 on avg), max depth = 18, in 5.058s\n",
      "[72/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.857s\n",
      "[73/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 5.416s\n",
      "[74/200] 26 trees, 806 leaves (31 on avg), max depth = 20, in 6.091s\n",
      "[75/200] 26 trees, 806 leaves (31 on avg), max depth = 18, in 4.791s\n",
      "[76/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.934s\n",
      "[77/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.815s\n",
      "[78/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 5.003s\n",
      "[79/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 5.251s\n",
      "[80/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.845s\n",
      "[81/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 5.854s\n",
      "[82/200] 26 trees, 806 leaves (31 on avg), max depth = 20, in 6.423s\n",
      "[83/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.703s\n",
      "[84/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 5.599s\n",
      "[85/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 5.412s\n",
      "[86/200] 26 trees, 806 leaves (31 on avg), max depth = 19, in 5.196s\n",
      "[87/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 5.020s\n",
      "[88/200] 26 trees, 806 leaves (31 on avg), max depth = 19, in 4.545s\n",
      "[89/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 5.068s\n",
      "[90/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.244s\n",
      "[91/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.187s\n",
      "[92/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.262s\n",
      "[93/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.211s\n",
      "[94/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 5.164s\n",
      "[95/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.310s\n",
      "[96/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.278s\n",
      "[97/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.206s\n",
      "[98/200] 26 trees, 806 leaves (31 on avg), max depth = 18, in 4.538s\n",
      "[99/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.830s\n",
      "[100/200] 26 trees, 806 leaves (31 on avg), max depth = 18, in 4.332s\n",
      "[101/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.351s\n",
      "[102/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.272s\n",
      "[103/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.251s\n",
      "[104/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.332s\n",
      "[105/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.194s\n",
      "[106/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.215s\n",
      "[107/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.398s\n",
      "[108/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.473s\n",
      "[109/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.417s\n",
      "[110/200] 26 trees, 806 leaves (31 on avg), max depth = 13, in 5.485s\n",
      "[111/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.460s\n",
      "[112/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.445s\n",
      "[113/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 5.234s\n",
      "[114/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.597s\n",
      "[115/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 5.483s\n",
      "[116/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.512s\n",
      "[117/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 5.293s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[118/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 5.408s\n",
      "[119/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.665s\n",
      "[120/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 5.389s\n",
      "[121/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.714s\n",
      "[122/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 5.071s\n",
      "[123/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.182s\n",
      "[124/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.882s\n",
      "[125/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 5.050s\n",
      "[126/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.514s\n",
      "[127/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 5.921s\n",
      "[128/200] 26 trees, 806 leaves (31 on avg), max depth = 13, in 4.778s\n",
      "[129/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 6.908s\n",
      "[130/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 6.358s\n",
      "[131/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 5.100s\n",
      "[132/200] 26 trees, 806 leaves (31 on avg), max depth = 19, in 4.575s\n",
      "[133/200] 26 trees, 806 leaves (31 on avg), max depth = 18, in 4.166s\n",
      "[134/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.129s\n",
      "[135/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.578s\n",
      "[136/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.393s\n",
      "[137/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.368s\n",
      "[138/200] 26 trees, 806 leaves (31 on avg), max depth = 13, in 4.381s\n",
      "[139/200] 26 trees, 806 leaves (31 on avg), max depth = 19, in 4.399s\n",
      "[140/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.313s\n",
      "[141/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.529s\n",
      "[142/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.271s\n",
      "[143/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.321s\n",
      "[144/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.308s\n",
      "[145/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.369s\n",
      "[146/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.547s\n",
      "[147/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.460s\n",
      "[148/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.613s\n",
      "[149/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.831s\n",
      "[150/200] 26 trees, 806 leaves (31 on avg), max depth = 18, in 4.642s\n",
      "[151/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.480s\n",
      "[152/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.217s\n",
      "[153/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.436s\n",
      "[154/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.471s\n",
      "[155/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.363s\n",
      "[156/200] 26 trees, 806 leaves (31 on avg), max depth = 13, in 4.340s\n",
      "[157/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.401s\n",
      "[158/200] 26 trees, 806 leaves (31 on avg), max depth = 13, in 4.248s\n",
      "[159/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.391s\n",
      "[160/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.306s\n",
      "[161/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.432s\n",
      "[162/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.215s\n",
      "[163/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.628s\n",
      "[164/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.605s\n",
      "[165/200] 26 trees, 806 leaves (31 on avg), max depth = 13, in 4.583s\n",
      "[166/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.333s\n",
      "[167/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.621s\n",
      "[168/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 5.002s\n",
      "[169/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 5.032s\n",
      "[170/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.353s\n",
      "[171/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.436s\n",
      "[172/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.379s\n",
      "[173/200] 26 trees, 806 leaves (31 on avg), max depth = 18, in 4.208s\n",
      "[174/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.361s\n",
      "[175/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.216s\n",
      "[176/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.722s\n",
      "[177/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.300s\n",
      "[178/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.547s\n",
      "[179/200] 26 trees, 806 leaves (31 on avg), max depth = 18, in 4.672s\n",
      "[180/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.460s\n",
      "[181/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.280s\n",
      "[182/200] 26 trees, 806 leaves (31 on avg), max depth = 13, in 4.336s\n",
      "[183/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.943s\n",
      "[184/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.782s\n",
      "[185/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.280s\n",
      "[186/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.686s\n",
      "[187/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.598s\n",
      "[188/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.262s\n",
      "[189/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.352s\n",
      "[190/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.539s\n",
      "[191/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 5.088s\n",
      "[192/200] 26 trees, 806 leaves (31 on avg), max depth = 14, in 4.467s\n",
      "[193/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 5.180s\n",
      "[194/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.584s\n",
      "[195/200] 26 trees, 806 leaves (31 on avg), max depth = 13, in 4.445s\n",
      "[196/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.494s\n",
      "[197/200] 26 trees, 806 leaves (31 on avg), max depth = 16, in 4.679s\n",
      "[198/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.811s\n",
      "[199/200] 26 trees, 806 leaves (31 on avg), max depth = 17, in 4.223s\n",
      "[200/200] 26 trees, 806 leaves (31 on avg), max depth = 15, in 4.125s\n",
      "Fit 5200 trees in 899.067 s, (161200 total leaves)\n",
      "Time spent computing histograms: 608.486s\n",
      "Time spent finding best splits:  98.696s\n",
      "Time spent applying splits:      13.390s\n",
      "Time spent predicting:           1.219s\n",
      "Accuracy 0.8914414414414414\n",
      "(14800,) [12 14 10 15 12  1  1 18  3  7]\n"
     ]
    }
   ],
   "source": [
    "# добавил условие не прекращать перебор, если приблизимся к нужной точности. Результат - стало еще точнее\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_scaled, y, test_size=.15, random_state=10)\n",
    "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "clf2 = HistGradientBoostingClassifier(\n",
    "    random_state=10, verbose=1, max_iter=200, n_iter_no_change=20, early_stopping=False\n",
    ")\n",
    "clf2.fit(X_train, y_train)\n",
    "y_pred = clf2.predict(X_val)\n",
    "acc = accuracy_score(y_val, y_pred)\n",
    "print(f'Accuracy', acc)\n",
    "\n",
    "y_test_pred = clf2.predict(X_test_scaled)\n",
    "print(y_test_pred.shape, y_test_pred[:10])\n",
    "\n",
    "submission = pd.DataFrame({'Id': np.arange(1, X_test_scaled.shape[0] + 1), 'Category': y_test_pred})\n",
    "submission.to_csv(f'jds3/kaggle_mnist_no_pca_hist_grad_b_std_idf_tf_200iter.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "k: 0 TRAIN: [17760 17761 17762 ... 88797 88798 88799] VALIDATION: [    0     1     2 ... 17757 17758 17759]\n",
      "(71040, 737) (17760, 737) (71040,) (17760,)\n",
      "Binning 0.377 GB of training data: 2.303 s\n",
      "Binning 0.042 GB of validation data: 0.064 s\n",
      "Fitting gradient boosted rounds:\n",
      "[1/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 2.13623, val loss: 2.20052, in 3.917s\n",
      "[2/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 1.82561, val loss: 1.92874, in 2.910s\n",
      "[3/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 1.61081, val loss: 1.73823, in 3.035s\n",
      "[4/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 1.44309, val loss: 1.58754, in 3.096s\n",
      "[5/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 1.30512, val loss: 1.46493, in 3.250s\n",
      "[6/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 1.18824, val loss: 1.36101, in 3.500s\n",
      "[7/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 1.08917, val loss: 1.27240, in 4.065s\n",
      "[8/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 1.00299, val loss: 1.19542, in 3.769s\n",
      "[9/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.92740, val loss: 1.12890, in 3.734s\n",
      "[10/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.86176, val loss: 1.07083, in 3.824s\n",
      "[11/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.80229, val loss: 1.01805, in 4.071s\n",
      "[12/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.75049, val loss: 0.97267, in 3.934s\n",
      "[13/1000] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.70370, val loss: 0.92971, in 3.899s\n",
      "[14/1000] 26 trees, 806 leaves (31 on avg), max depth = 20, train loss: 0.66096, val loss: 0.89315, in 4.030s\n",
      "[15/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.62245, val loss: 0.86026, in 5.617s\n",
      "[16/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.58751, val loss: 0.83164, in 6.263s\n",
      "[17/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.55581, val loss: 0.80403, in 3.876s\n",
      "[18/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.52730, val loss: 0.78052, in 4.108s\n",
      "[19/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.50055, val loss: 0.75898, in 3.959s\n",
      "[20/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.47548, val loss: 0.73810, in 4.227s\n",
      "[21/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.45279, val loss: 0.72068, in 4.051s\n",
      "[22/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.43093, val loss: 0.70338, in 4.081s\n",
      "[23/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.41075, val loss: 0.68857, in 4.312s\n",
      "[24/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.39170, val loss: 0.67568, in 4.021s\n",
      "[25/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.37419, val loss: 0.66184, in 4.484s\n",
      "[26/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.35758, val loss: 0.64877, in 5.432s\n",
      "[27/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.34193, val loss: 0.63720, in 7.232s\n",
      "[28/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.32759, val loss: 0.62625, in 5.059s\n",
      "[29/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.31367, val loss: 0.61567, in 6.081s\n",
      "[30/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.30032, val loss: 0.60554, in 4.187s\n",
      "[31/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.28790, val loss: 0.59725, in 4.675s\n",
      "[32/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.27716, val loss: 0.59198, in 4.192s\n",
      "[33/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.26602, val loss: 0.58538, in 4.403s\n",
      "[34/1000] 26 trees, 806 leaves (31 on avg), max depth = 13, train loss: 0.25529, val loss: 0.57801, in 6.489s\n",
      "[35/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.24520, val loss: 0.57138, in 4.268s\n",
      "[36/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.23582, val loss: 0.56535, in 5.297s\n",
      "[37/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.22665, val loss: 0.55814, in 4.982s\n",
      "[38/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.21818, val loss: 0.55232, in 5.053s\n",
      "[39/1000] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.21007, val loss: 0.54641, in 5.027s\n",
      "[40/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.20248, val loss: 0.54106, in 3.943s\n",
      "[41/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.19527, val loss: 0.53729, in 3.951s\n",
      "[42/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.18839, val loss: 0.53218, in 3.880s\n",
      "[43/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.18143, val loss: 0.52671, in 4.018s\n",
      "[44/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.17502, val loss: 0.52276, in 3.993s\n",
      "[45/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.16888, val loss: 0.51756, in 4.483s\n",
      "[46/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.16303, val loss: 0.51307, in 5.364s\n",
      "[47/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.15754, val loss: 0.50900, in 5.235s\n",
      "[48/1000] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.15220, val loss: 0.50511, in 7.432s\n",
      "[49/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.14703, val loss: 0.50118, in 15.955s\n",
      "[50/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.14202, val loss: 0.49733, in 6.090s\n",
      "[51/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.13744, val loss: 0.49422, in 4.625s\n",
      "[52/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.13285, val loss: 0.49069, in 4.796s\n",
      "[53/1000] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.12857, val loss: 0.48791, in 6.692s\n",
      "[54/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.12436, val loss: 0.48460, in 7.764s\n",
      "[55/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.12037, val loss: 0.48150, in 7.510s\n",
      "[56/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.11662, val loss: 0.47883, in 6.054s\n",
      "[57/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.11297, val loss: 0.47586, in 7.753s\n",
      "[58/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.10944, val loss: 0.47346, in 8.060s\n",
      "[59/1000] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.10590, val loss: 0.47081, in 8.212s\n",
      "[60/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.10265, val loss: 0.46861, in 9.275s\n",
      "[61/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.09943, val loss: 0.46629, in 6.011s\n",
      "[62/1000] 26 trees, 806 leaves (31 on avg), max depth = 20, train loss: 0.09649, val loss: 0.46391, in 9.155s\n",
      "[63/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.09361, val loss: 0.46159, in 4.613s\n",
      "[64/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.09079, val loss: 0.45912, in 4.283s\n",
      "[65/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.08809, val loss: 0.45645, in 4.851s\n",
      "[66/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.08551, val loss: 0.45435, in 4.773s\n",
      "[67/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.08294, val loss: 0.45220, in 4.452s\n",
      "[68/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.08052, val loss: 0.45011, in 4.439s\n",
      "[69/1000] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.07821, val loss: 0.44842, in 4.403s\n",
      "[70/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.07586, val loss: 0.44684, in 4.855s\n",
      "[71/1000] 26 trees, 806 leaves (31 on avg), max depth = 20, train loss: 0.07365, val loss: 0.44530, in 4.631s\n",
      "[72/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.07156, val loss: 0.44388, in 7.834s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[73/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.06952, val loss: 0.44230, in 6.510s\n",
      "[74/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.06760, val loss: 0.44076, in 13.946s\n",
      "[75/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.06574, val loss: 0.43902, in 8.642s\n",
      "[76/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.06393, val loss: 0.43740, in 7.357s\n",
      "[77/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.06224, val loss: 0.43613, in 12.564s\n",
      "[78/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.06058, val loss: 0.43492, in 5.574s\n",
      "[79/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.05884, val loss: 0.43306, in 4.372s\n",
      "[80/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.05719, val loss: 0.43170, in 4.724s\n",
      "[81/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.05566, val loss: 0.43040, in 6.836s\n",
      "[82/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.05417, val loss: 0.42920, in 11.811s\n",
      "[83/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.05271, val loss: 0.42819, in 9.419s\n",
      "[84/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.05132, val loss: 0.42699, in 11.685s\n",
      "[85/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.04986, val loss: 0.42574, in 11.859s\n",
      "[86/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.04852, val loss: 0.42451, in 6.403s\n",
      "[87/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.04718, val loss: 0.42323, in 6.792s\n",
      "[88/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.04602, val loss: 0.42231, in 5.204s\n",
      "[89/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.04486, val loss: 0.42179, in 11.413s\n",
      "[90/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.04372, val loss: 0.42090, in 7.053s\n",
      "[91/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.04264, val loss: 0.41978, in 11.117s\n",
      "[92/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.04160, val loss: 0.41906, in 5.743s\n",
      "[93/1000] 26 trees, 806 leaves (31 on avg), max depth = 20, train loss: 0.04056, val loss: 0.41796, in 8.944s\n",
      "[94/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.03946, val loss: 0.41686, in 6.866s\n",
      "[95/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.03847, val loss: 0.41605, in 7.879s\n",
      "[96/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.03757, val loss: 0.41484, in 5.935s\n",
      "[97/1000] 26 trees, 806 leaves (31 on avg), max depth = 20, train loss: 0.03668, val loss: 0.41443, in 5.737s\n",
      "[98/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.03582, val loss: 0.41399, in 5.382s\n",
      "[99/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.03490, val loss: 0.41309, in 9.201s\n",
      "[100/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.03406, val loss: 0.41264, in 9.998s\n",
      "[101/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.03324, val loss: 0.41191, in 5.810s\n",
      "[102/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.03248, val loss: 0.41107, in 8.680s\n",
      "[103/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.03171, val loss: 0.41050, in 5.741s\n",
      "[104/1000] 26 trees, 806 leaves (31 on avg), max depth = 20, train loss: 0.03091, val loss: 0.40976, in 5.825s\n",
      "[105/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.03009, val loss: 0.40888, in 7.707s\n",
      "[106/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.02940, val loss: 0.40803, in 9.474s\n",
      "[107/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.02870, val loss: 0.40700, in 4.846s\n",
      "[108/1000] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.02800, val loss: 0.40648, in 7.052s\n",
      "[109/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.02737, val loss: 0.40605, in 7.823s\n",
      "[110/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.02669, val loss: 0.40539, in 6.525s\n",
      "[111/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.02604, val loss: 0.40503, in 8.999s\n",
      "[112/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.02545, val loss: 0.40458, in 8.189s\n",
      "[113/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.02485, val loss: 0.40403, in 9.170s\n",
      "[114/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.02428, val loss: 0.40379, in 7.769s\n",
      "[115/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.02373, val loss: 0.40335, in 9.890s\n",
      "[116/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.02320, val loss: 0.40299, in 9.454s\n",
      "[117/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.02267, val loss: 0.40281, in 7.084s\n",
      "[118/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.02215, val loss: 0.40242, in 4.656s\n",
      "[119/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.02164, val loss: 0.40180, in 7.087s\n",
      "[120/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.02111, val loss: 0.40158, in 10.871s\n",
      "[121/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.02063, val loss: 0.40145, in 6.034s\n",
      "[122/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.02014, val loss: 0.40104, in 5.998s\n",
      "[123/1000] 26 trees, 806 leaves (31 on avg), max depth = 22, train loss: 0.01968, val loss: 0.40077, in 5.873s\n",
      "[124/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01926, val loss: 0.40016, in 8.465s\n",
      "[125/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01887, val loss: 0.40020, in 7.332s\n",
      "[126/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.01844, val loss: 0.39961, in 16.531s\n",
      "[127/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01804, val loss: 0.39936, in 6.797s\n",
      "[128/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01761, val loss: 0.39942, in 5.859s\n",
      "[129/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01719, val loss: 0.39904, in 11.059s\n",
      "[130/1000] 26 trees, 806 leaves (31 on avg), max depth = 12, train loss: 0.01682, val loss: 0.39862, in 4.298s\n",
      "[131/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01645, val loss: 0.39812, in 3.936s\n",
      "[132/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.01609, val loss: 0.39819, in 3.971s\n",
      "[133/1000] 26 trees, 806 leaves (31 on avg), max depth = 19, train loss: 0.01572, val loss: 0.39774, in 3.815s\n",
      "[134/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01534, val loss: 0.39725, in 4.895s\n",
      "[135/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.01500, val loss: 0.39727, in 5.064s\n",
      "[136/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.01465, val loss: 0.39715, in 4.810s\n",
      "[137/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01430, val loss: 0.39707, in 7.756s\n",
      "[138/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.01403, val loss: 0.39677, in 11.036s\n",
      "[139/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 0.01372, val loss: 0.39668, in 12.932s\n",
      "[140/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01342, val loss: 0.39640, in 12.066s\n",
      "[141/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01312, val loss: 0.39611, in 8.229s\n",
      "[142/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01286, val loss: 0.39591, in 5.761s\n",
      "[143/1000] 26 trees, 806 leaves (31 on avg), max depth = 14, train loss: 0.01258, val loss: 0.39586, in 4.472s\n",
      "[144/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.01232, val loss: 0.39607, in 7.310s\n",
      "[145/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01207, val loss: 0.39617, in 11.573s\n",
      "[146/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01182, val loss: 0.39620, in 5.714s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[147/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01155, val loss: 0.39622, in 8.660s\n",
      "[148/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01129, val loss: 0.39610, in 6.045s\n",
      "[149/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01107, val loss: 0.39598, in 5.537s\n",
      "[150/1000] 26 trees, 806 leaves (31 on avg), max depth = 15, train loss: 0.01081, val loss: 0.39607, in 5.781s\n",
      "[151/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 0.01056, val loss: 0.39593, in 4.943s\n",
      "[152/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01034, val loss: 0.39607, in 5.922s\n",
      "[153/1000] 26 trees, 806 leaves (31 on avg), max depth = 16, train loss: 0.01013, val loss: 0.39612, in 5.927s\n",
      "Fit 3978 trees in 990.330 s, (123318 total leaves)\n",
      "Time spent computing histograms: 657.119s\n",
      "Time spent finding best splits:  143.633s\n",
      "Time spent applying splits:      37.441s\n",
      "Time spent predicting:           1.894s\n",
      "\n",
      "Accuracy 0.8834459459459459\n",
      "\n",
      "k: 1 TRAIN: [    0     1     2 ... 88797 88798 88799] VALIDATION: [17760 17761 17762 ... 35517 35518 35519]\n",
      "(71040, 737) (17760, 737) (71040,) (17760,)\n",
      "Binning 0.377 GB of training data: 3.081 s\n",
      "Binning 0.042 GB of validation data: 0.083 s\n",
      "Fitting gradient boosted rounds:\n",
      "[154/1000] 26 trees, 806 leaves (31 on avg), max depth = 17, train loss: 5.25309, val loss: 5.38930, in 3.330s\n",
      "[155/1000] 26 trees, 806 leaves (31 on avg), max depth = 18, train loss: 50.95176, val loss: 56.21854, in 2.450s\n",
      "[156/1000] 26 trees, 806 leaves (31 on avg), max depth = 22, train loss: 80.96447, val loss: 91.19416, in 2.443s\n",
      "[157/1000] 26 trees, 806 leaves (31 on avg), max depth = 25, train loss: 137.15923, val loss: 149.20131, in 2.495s\n",
      "[158/1000] 26 trees, 806 leaves (31 on avg), max depth = 27, train loss: 160.29311, val loss: 174.64358, in 2.490s\n",
      "[159/1000] 26 trees, 806 leaves (31 on avg), max depth = 28, train loss: 236.43411, val loss: 254.26387, in 2.668s\n",
      "[160/1000] 26 trees, 806 leaves (31 on avg), max depth = 27, train loss: 325.79336, val loss: 357.49678, in 2.503s\n",
      "[161/1000] 26 trees, 806 leaves (31 on avg), max depth = 26, train loss: 469.61593, val loss: 482.27588, in 2.616s\n",
      "Fit 4186 trees in 29.255 s, (129766 total leaves)\n",
      "Time spent computing histograms: 11.489s\n",
      "Time spent finding best splits:  2.598s\n",
      "Time spent applying splits:      0.557s\n",
      "Time spent predicting:           0.035s\n",
      "\n",
      "Accuracy 0.7688063063063063\n",
      "\n",
      "k: 2 TRAIN: [    0     1     2 ... 88797 88798 88799] VALIDATION: [35520 35521 35522 ... 53277 53278 53279]\n",
      "(71040, 737) (17760, 737) (71040,) (17760,)\n",
      "Binning 0.377 GB of training data: 2.668 s\n",
      "Binning 0.042 GB of validation data: 0.061 s\n",
      "Fitting gradient boosted rounds:\n",
      "[162/1000] 26 trees, 806 leaves (31 on avg), max depth = 26, train loss: 1281.46865, val loss: 1382.32161, in 2.934s\n",
      "Fit 4212 trees in 10.942 s, (130572 total leaves)\n",
      "Time spent computing histograms: 1.732s\n",
      "Time spent finding best splits:  0.308s\n",
      "Time spent applying splits:      0.123s\n",
      "Time spent predicting:           0.004s\n",
      "\n",
      "Accuracy 0.6287725225225225\n",
      "\n",
      "k: 3 TRAIN: [    0     1     2 ... 88797 88798 88799] VALIDATION: [53280 53281 53282 ... 71037 71038 71039]\n",
      "(71040, 737) (17760, 737) (71040,) (17760,)\n",
      "Binning 0.377 GB of training data: 2.569 s\n",
      "Binning 0.042 GB of validation data: 0.057 s\n",
      "Fitting gradient boosted rounds:\n",
      "[163/1000] 26 trees, 806 leaves (31 on avg), max depth = 27, train loss: 2545.04087, val loss: 2661.90810, in 3.455s\n",
      "Fit 4238 trees in 11.089 s, (131378 total leaves)\n",
      "Time spent computing histograms: 2.278s\n",
      "Time spent finding best splits:  0.303s\n",
      "Time spent applying splits:      0.075s\n",
      "Time spent predicting:           0.006s\n",
      "\n",
      "Accuracy 0.5537725225225225\n",
      "\n",
      "k: 4 TRAIN: [    0     1     2 ... 71037 71038 71039] VALIDATION: [71040 71041 71042 ... 88797 88798 88799]\n",
      "(71040, 737) (17760, 737) (71040,) (17760,)\n",
      "Binning 0.377 GB of training data: 2.636 s\n",
      "Binning 0.042 GB of validation data: 0.106 s\n",
      "Fitting gradient boosted rounds:\n",
      "[164/1000] 26 trees, 806 leaves (31 on avg), max depth = 25, train loss: 7098.53458, val loss: 7345.78754, in 3.682s\n",
      "Fit 4264 trees in 12.081 s, (132184 total leaves)\n",
      "Time spent computing histograms: 2.357s\n",
      "Time spent finding best splits:  0.447s\n",
      "Time spent applying splits:      0.067s\n",
      "Time spent predicting:           0.005s\n",
      "\n",
      "Accuracy 0.42100225225225224\n"
     ]
    }
   ],
   "source": [
    "# градиентный бустинг, обучаемый через кросс-валидацию k-fold\n",
    "# 1000 итераций - по 200 в каждом пакете k\n",
    "clf2 = HistGradientBoostingClassifier(\n",
    "    random_state=10, verbose=1, max_iter=1000, warm_start=True\n",
    ")\n",
    "kf = KFold(n_splits=5)\n",
    "\n",
    "for k, (train_index, val_index) in enumerate(kf.split(X_train_scaled)):\n",
    "    print('\\nk:', k, \"TRAIN:\", train_index, \"VALIDATION:\", val_index)\n",
    "    X_train, X_val = X_train_scaled[train_index], X_train_scaled[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n",
    "\n",
    "    clf2.fit(X_train, y_train)\n",
    "    y_pred = clf2.predict(X_val)\n",
    "    acc = accuracy_score(y_val, y_pred)\n",
    "    print(f'\\nAccuracy', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# все плохо. Похоже, что алгоритм переобучается при 150 итерациях."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Отчет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Работа над данной задачей была выполнена в 3 \"захода\". Данный ноутбук отражает третью итерацию.\n",
    "\n",
    "В первой итерации я перепробовал стандартные классификаторы, предложенные по заданию. Точнее всего предсказывали SVC и k-neighbours. SVC давал отличный результат, но тренировался очень медленно на моей машине и тем более в colab. А так как я начал работать поздно, то пришлось от нее отказаться. Но еще лучше показал себя RandomForest. После подбора гиперпараметров попробовал данные модели с наилучшими гиперпараметрами, но точность осталась в районе 0.75. Как оказалось потом, я забыл разделить тестовое множество на 255, поэтому предсказания на тестовой выборке давали очень низкий результат.\n",
    "\n",
    "Во второй итерации решил поработать с входными данными векторизовать изображения, чтобы убрать весь шум. Но тут появились проблемы, связанные с векторизацией. Скелетонизация \"съедала\" \"закорючки\" рукописного воода и слабое нажатие. Векторизация скелетона еще сильнее уменьшала объем исходной информации. Но главная проблема, скорее всего, была в том, что нужно было подобрать качественные метрики для найденных векторов, которые бы собрали сигнатуру входных примеров так, чтобы можно было определить \"похожесть\" максимально эффективно. Навыков не хватило, результат не более 0,65. https://colab.research.google.com/drive/13sTcHhWMHVAnNm5XqxOj9V-SITQ7ERZo?usp=sharing\n",
    "\n",
    "В третьей итерации решил поработать плотнее с ансамблевыми методами.\n",
    "Первым этапом свалидировал эффективность обработки входных данных. Как оказалось, приведение изображения к черно-белому (1 или 0) уменьшало точность. Видимо, линия рукописного ввода становилась толще. Скорее всего, уменьшение порога преобразования (эродирование границы) помогло бы, но я решил для экономии времени оставить полутона, преобразовав оттенки серого в значения от 0 до 1 и пропустив их через StandardScaler. Данная трансформация ни в одном примере не ухудшала результат.\n",
    "\n",
    "Далее приступил к подбору ансамлевых методов. Часть из них не удалось параметризовать (выдавали ошибки в процессе тренировки, например AdaBoostClassifier), часть тренировалась слишком медленно. С помощью предварительной оценки точночти выбрал классификатор градиентного бустинга на основе гистограмм (HistGradientBoostingClassifier). Этот классификатор мне очень понравился, в первую очередь своим удобным логом. В логах на каждой итерации отражались потери тренировочной и валидирующей выборки. Поиграв немного с параметрами, убедился в том, что стандартные настройки довольно неплохи, поэтому не стал забивать алгоритм ограничениями. Дав модели больше итераций на обработку, увеличил результат до точности 0,883.\n",
    "\n",
    "Далее попробовал PCA для уменьшения размерности. Но, похоже, что он лучше походит для SVC, в любых алгоритмах деревьев решений он ухудшал результат.\n",
    "\n",
    "В конце решил воспользоваться еще тренировкой через кросс-валидацию. В HistGradientBoostingClassifier это достигается через параметр warm_start. Однако, результат при обучении через кросс-валидацию ухудшается. Такой вариант не подходит, или я что-то делаю не так.\n",
    "Мой финальный результат - 0,883."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MNIST_Kaggle.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}